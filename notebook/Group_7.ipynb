{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Required Packages\n",
        "\n",
        "- `langchain`: The core library for creating LLM-based applications. It offers tools for document loading, splitting, embedding, vector storage, memory, and more.\n",
        "- `langchain-community`: Contains community-maintained integrations for document loaders, vector stores, and models.\n",
        "- `langchain-mistralai`: Provides access to **Mistral LLMs** via LangChain, enabling us to use Mistral as our language model backend.\n",
        "- `chromadb`: A lightweight and fast vector database used to store and retrieve document embeddings efficiently. It's a key component of our retrieval system.\n",
        "- `gradio`: A UI library that allows us to build an interactive web interface where users can type questions and receive AI-generated answers from our fairy tale chatbot.\n",
        "\n",
        "By installing these packages, we’re setting up the software environment to support every core function of the RAG architecture: **data ingestion, preprocessing, embedding, retrieval, generation, and user interaction**.\n"
      ],
      "metadata": {
        "id": "O-nZAu2m8Dp6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "g1lvhC-AJZ5q"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain langchain-community langchain-mistralai chromadb gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Required Modules\n",
        "\n",
        "This cell imports all the core libraries and components needed to build the Retrieval-Augmented Generation (RAG) chatbot pipeline.\n",
        "\n",
        "\n",
        "- `PyPDFLoader`:  \n",
        "  Loads and parses PDF files. This is used to ingest the fairy tale documents into the system.\n",
        "\n",
        "- `RecursiveCharacterTextSplitter`:  \n",
        "  Splits long text documents into smaller, overlapping chunks. This improves retrieval quality and helps ensure that each chunk is within the token limit for embedding.\n",
        "\n",
        "- `Chroma`:  \n",
        "  A vector database used to store and retrieve text embeddings. It performs similarity searches when a user asks a question.\n",
        "\n",
        "- `HuggingFaceEmbeddings`:  \n",
        "  Transforms text into vector embeddings using a transformer-based model from Hugging Face.\n",
        "\n",
        "- `ConversationBufferMemory`:  \n",
        "  Maintains the conversation history, allowing the chatbot to respond with context-aware answers over multiple turns.\n",
        "\n",
        "- `ConversationalRetrievalChain`:  \n",
        "  A LangChain component that combines document retrieval and LLM-based answer generation in a single chain.\n",
        "\n",
        "- `ChatMistralAI`:  \n",
        "  Provides an interface to the Mistral large language model via API, which is used to generate answers to user queries.\n",
        "\n",
        "- `gradio`:  \n",
        "  A Python library for building interactive web UIs. This will be used to create the chatbot interface.\n",
        "\n",
        "- `os`:  \n",
        "  Standard Python module used here to handle environment variables, such as setting the API key securely.\n",
        "\n",
        "These components form the foundational building blocks of the RAG system: loading, splitting, embedding, storing, retrieving, and responding to user queries.\n"
      ],
      "metadata": {
        "id": "HrlWqZxk-DId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_mistralai.chat_models import ChatMistralAI\n",
        "import gradio as gr\n",
        "import os"
      ],
      "metadata": {
        "id": "DyckJZ_TJhIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting the Mistral API Key and Initializing the Language Model\n",
        "\n",
        "This cell configures access to the Mistral language model by setting the required API key and initializing the model.\n",
        "\n",
        "\n",
        "- `os.environ[\"MISTRAL_API_KEY\"] = \"key\"`  \n",
        "  This line sets the `MISTRAL_API_KEY` environment variable, which is used to authenticate requests made to the Mistral API.  \n",
        "  In production or shared environments, the actual key should be stored securely and not hardcoded.\n",
        "\n",
        "- `ChatMistralAI(model=\"mistral-small\", temperature=0)`  \n",
        "  Initializes the Mistral language model with the specified parameters:\n",
        "  - `model=\"mistral-small\"` refers to the specific model variant being used.\n",
        "  - `temperature=0` sets the randomness of the output to zero, making the model’s responses more deterministic and consistent.\n",
        "\n",
        "This model will be used later in the pipeline to generate natural language responses based on the user’s query and the retrieved document content.\n"
      ],
      "metadata": {
        "id": "ypDDd40r_Lv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"MISTRAL_API_KEY\"] = \"ZYxgsoPPGqJYjHIGoCIEbP1vcxbNJstX\"\n",
        "mistral_llm = ChatMistralAI(model=\"mistral-small\", temperature=0)\n"
      ],
      "metadata": {
        "id": "f_fIe7vpKIjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing PDF Parser and Loading Documents\n",
        "\n",
        "This cell performs two key actions: installing a PDF parsing library and loading the fairy tale PDF documents from a directory.\n",
        "\n",
        "\n",
        "\n",
        "- `!pip install pypdf`  \n",
        "  Installs the `pypdf` library, which is a dependency for parsing and reading PDF files. This is required for LangChain’s PDF loaders to function correctly.\n",
        "\n",
        "- `from langchain_community.document_loaders import PyPDFDirectoryLoader`  \n",
        "  Imports the `PyPDFDirectoryLoader` class, which allows batch loading of all PDF files from a specified folder.\n",
        "\n",
        "- `loader = PyPDFDirectoryLoader(\"/content\")`  \n",
        "  Creates a document loader instance targeting the `/content` directory (default working directory in Google Colab). All PDF files placed in this folder will be read.\n",
        "\n",
        "- `documents = loader.load()`  \n",
        "  Loads and parses the PDF files into a list of LangChain `Document` objects. Each document contains:\n",
        "  - The textual content extracted from the PDF\n",
        "  - Metadata such as the file name and page number\n",
        "\n",
        "These documents will later be split into chunks, embedded into vectors, and stored in a vector database for retrieval during chatbot interactions.\n"
      ],
      "metadata": {
        "id": "l4K_rIrV_3pC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-d8Z0RzKvY0",
        "outputId": "eaeebcad-9213-4b7b-e69e-5a9c9eb70a5c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "loader = PyPDFDirectoryLoader(\"/content\")\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "Vhz7Osb2Kdqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting Documents into Chunks\n",
        "\n",
        "This cell splits the loaded documents into smaller text chunks using LangChain’s text splitter utility. Splitting is essential for efficient embedding and retrieval in a RAG pipeline.\n",
        "\n",
        "- `RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)`  \n",
        "  Initializes a text splitter that breaks large documents into smaller segments of up to 500 characters each.  \n",
        "  The `chunk_overlap=50` means that 65 characters from the end of one chunk are repeated at the start of the next chunk.  \n",
        "  This overlapping technique helps preserve context and avoids cutting off important information at chunk boundaries.\n",
        "\n",
        "- `docs = splitter.split_documents(documents)`  \n",
        "  Applies the text splitter to the previously loaded `documents`.  \n",
        "  The result is a list of smaller, manageable text chunks stored in `docs`. Each chunk retains the original document’s metadata.\n",
        "\n",
        "Splitting documents is a critical preprocessing step in RAG systems. It ensures that:\n",
        "- The input size fits within token limits of embedding models and LLMs\n",
        "- Retrieval is more fine-grained and contextually relevant\n"
      ],
      "metadata": {
        "id": "lRTcTgS2BlWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "docs = splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "UbC1lAVHKlMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating Embeddings and Initializing Vector Store\n",
        "\n",
        "This cell sets up the document embedding model and initializes the vector database for efficient semantic search.\n",
        "\n",
        "\n",
        "We use `HuggingFaceEmbeddings` with the pre-trained model `\"all-MiniLM-L6-v2\"` to convert each chunk of text into a dense numerical vector. This model is widely used due to its excellent trade-off between speed and semantic performance. It captures sentence-level meaning and is lightweight enough for real-time inference on consumer-grade hardware. Hugging Face models also run locally and are open-source, making them cost-effective and flexible for academic and prototype projects.\n",
        "\n",
        "The vector store is implemented using `Chroma`, which supports fast in-memory and persistent storage of vector data. `Chroma.from_documents()` takes the list of preprocessed document chunks and their embeddings, storing them internally for future retrieval. Finally, we extract a `retriever` object from the vector store using `.as_retriever()`, which enables semantic similarity search based on user queries.\n",
        "\n",
        "This approach ensures that when a user asks a question, the system can find and retrieve the most relevant text chunks from the embedded knowledge base using cosine similarity.\n"
      ],
      "metadata": {
        "id": "NuN1anT5s2Hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "vectordb = Chroma.from_documents(docs, embedding=embedding)\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})"
      ],
      "metadata": {
        "id": "8egmjAK8K1kN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "044cbe55-f817-45fa-8ca5-ef7d9cffcf7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-9ed48cb5d2c5>:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Initializing Conversation Memory\n",
        "\n",
        "This cell sets up memory management for the chatbot using LangChain's `ConversationBufferMemory`. Memory is essential for maintaining the context of multi-turn conversations.\n",
        "\n",
        "\n",
        "\n",
        "- `ConversationBufferMemory(...)`  \n",
        "  Creates a memory object that stores the full conversation history in memory, enabling the chatbot to respond with awareness of prior exchanges. It helps produce more coherent and context-aware answers.\n",
        "\n",
        "#### Parameters:\n",
        "- `memory_key=\"chat_history\"`  \n",
        "  Defines the key used to store and retrieve past dialogue messages from memory.\n",
        "\n",
        "- `return_messages=True`  \n",
        "  Ensures that past messages are returned in their original message format (rather than raw text).\n",
        "\n",
        "- `output_key=\"answer\"`  \n",
        "  Specifies that the output of the retrieval-augmented generation pipeline will be stored under the key `\"answer\"` in memory.\n",
        "\n",
        "Using memory in RAG applications enhances the chatbot’s ability to hold meaningful conversations over multiple turns rather than responding in isolation.\n"
      ],
      "metadata": {
        "id": "3vOqjovhQEH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True,\n",
        "    output_key=\"answer\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "c-shm1HHK3bU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f55dde25-3529-4c52-9568-3ddfeae8c108"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-eccafb848086>:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the Conversational Retrieval Chain\n",
        "\n",
        "This cell sets up the core of the Retrieval-Augmented Generation (RAG) system using LangChain’s `ConversationalRetrievalChain`. This chain combines document retrieval with language model generation in a single, seamless interface.\n",
        "\n",
        "- `ConversationalRetrievalChain.from_llm(...)`  \n",
        "  This method initializes a conversation-aware QA system that can:\n",
        "  1. Retrieve relevant document chunks from the vector store based on the current user query\n",
        "  2. Combine that with prior conversation history (via memory)\n",
        "  3. Pass everything to the language model to generate a well-informed response\n",
        "\n",
        "#### Parameters:\n",
        "\n",
        "- `llm=mistral_llm`  \n",
        "  Specifies the language model to use for generating answers. In this case, it's the previously initialized Mistral model (`mistral-small`).\n",
        "\n",
        "- `retriever=retriever`  \n",
        "  Connects the retriever (typically a Chroma-based similarity search tool) that fetches relevant document chunks based on the user’s query.\n",
        "\n",
        "- `memory=memory`  \n",
        "  Injects the conversation memory created earlier. This allows the chain to handle multi-turn conversations and maintain context from earlier interactions.\n",
        "\n",
        "- `return_source_documents=True`  \n",
        "  Ensures that the original document chunks used for generating the answer are returned along with the answer itself. This is useful for transparency or debugging.\n",
        "\n",
        "- `output_key=\"answer\"`  \n",
        "  Defines the key under which the generated response will be stored and accessed.\n",
        "\n",
        "This chain becomes the brain of the chatbot, managing context, performing retrieval, and generating responses.\n"
      ],
      "metadata": {
        "id": "Z43x0ilJQep8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=mistral_llm,\n",
        "    retriever=retriever,\n",
        "    memory=memory,\n",
        "    return_source_documents=True,\n",
        "    output_key=\"answer\"\n",
        ")"
      ],
      "metadata": {
        "id": "XdQ0HuRvLH7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Gradio Chatbot Interface\n",
        "\n",
        "This cell defines the backend logic for the chatbot’s interactive user interface using Gradio. It handles user messages, determines appropriate responses, and invokes the RAG model to generate answers.\n",
        "\n",
        "\n",
        "- `respond_to_user(message, history)`  \n",
        "  This is the main callback function that gets triggered whenever a user sends a message through the Gradio chat interface. It processes the message, interacts with the QA pipeline, and returns the response.\n",
        "\n",
        "### Key Functional Steps:\n",
        "\n",
        "1. **Greeting and Exit Handling:**\n",
        "   - The input message is converted to lowercase and stripped of extra spaces.\n",
        "   - If the message is a greeting (like “hi”, “hello”, etc.), a friendly welcome message is returned.\n",
        "   - If the user wants to exit (e.g., “bye”, “goodbye”), a farewell message is shown.\n",
        "   - These are handled before invoking the QA chain.\n",
        "\n",
        "2. **Answer Retrieval:**\n",
        "   - The `qa_chain.invoke({\"question\": message})` call sends the user's message to the Conversational Retrieval Chain.\n",
        "   - The chain returns a dictionary with keys like `\"answer\"` or `\"result\"`, depending on the LLM used.\n",
        "   - The code safely retrieves the response using `.get()` to avoid key errors.\n",
        "\n",
        "3. **Fallback Handling:**\n",
        "   - If the answer indicates uncertainty (e.g., contains phrases like “don’t know” or “not sure”), a soft, encouraging fallback message is added.\n",
        "   - This keeps the user experience positive, even if the chatbot cannot find a good answer.\n",
        "\n",
        "4. **Exception Handling:**\n",
        "   - If an error occurs during execution, the traceback is printed for debugging, and a formatted error message is returned to the UI.\n",
        "\n",
        "This function enables dynamic interaction between users and the RAG model, supporting real-time Q&A with conversational memory and fallback safety.\n"
      ],
      "metadata": {
        "id": "edoN8j4CQ7S9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def respond_to_user(message, history):\n",
        "    try:\n",
        "        message_lower = message.lower().strip()\n",
        "        greetings = [\"hi\", \"hello\", \"hey\", \"good morning\", \"good evening\", \"what's up\", \"how are you\"]\n",
        "        exit_phrases = [\"bye\", \"goodbye\", \"see you later\", \"exit\"]\n",
        "\n",
        "        if message_lower in greetings:\n",
        "            return \"🧚‍♀️ Hello! Ask me anything about fairy tales and I’ll do my best to help!\"\n",
        "        if message_lower in exit_phrases:\n",
        "            return \"👋 Bye! Have a magical day! 🌟\"\n",
        "\n",
        "        response = qa_chain.invoke({\"question\": message})\n",
        "        answer = response.get(\"answer\", \"\") or response.get(\"result\", \"\")\n",
        "\n",
        "        if \"don't know\" in answer.lower() or \"not sure\" in answer.lower():\n",
        "          answer += \" 😊 I'm sorry, I don't know the answer to this question. But I'm always learning!\"\n",
        "\n",
        "        return answer + \" \\n🧙‍♀️Thanks for asking!\\n Do you want to ask anything else?\"\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(traceback.format_exc())\n",
        "        return f\"Error:\\n{str(e)}\"\n"
      ],
      "metadata": {
        "id": "IO0MY5JiLIi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the Gradio UI for the Fairy Tale Chatbot\n",
        "\n",
        "This cell builds a user-friendly web interface using `gradio.Blocks` for interacting with the fairy tale RAG chatbot. It includes a visual header, an optional image, and a conversational chat interface that calls the backend function `respond_to_user()`.\n",
        "\n",
        "\n",
        "- `with gr.Blocks(theme=gr.themes.Soft()) as demo:`  \n",
        "  Initializes a Gradio Blocks interface with a clean and modern visual theme. The `Soft` theme provides a light, user-friendly design.\n",
        "\n",
        "- `gr.Markdown(...)`  \n",
        "  Adds custom Markdown text to the interface:\n",
        "  - The first line serves as a title.\n",
        "  - The second line introduces the chatbot and what users can expect from it.\n",
        "\n",
        "- `gr.Image(\"/content/bg.gif\", height=278, width=500)`  \n",
        "  Displays a background or thematic image (e.g., a fairy-tale themed GIF). This makes the UI visually appealing and engaging for users.\n",
        "\n",
        "- `gr.ChatInterface(...)`  \n",
        "  Builds the core chat module using Gradio’s high-level chat wrapper. Key attributes include:\n",
        "  - `fn=respond_to_user`: Connects the user’s message input to the chatbot response function.\n",
        "  - `title`: Sets the chat window’s title.\n",
        "  - `description`: Provides instructions or context for the user.\n",
        "  - `examples`: Offers predefined example questions users can click on to get started.\n",
        "  - `type=\"messages\"`: Formats the input and output as chat-style messages.\n",
        "\n",
        "- `demo.launch(share=True, inline=False)`  \n",
        "  Launches the Gradio app.\n",
        "  - `share=True` generates a public link so the chatbot can be accessed and tested outside of the notebook.\n",
        "  - `inline=False` ensures that the interface opens in a new browser tab instead of embedding within the notebook.\n",
        "\n",
        "This interface makes it easy for users to have natural conversations with the fairy tale chatbot without needing to interact with raw code or command-line prompts."
      ],
      "metadata": {
        "id": "-KxVpJugVA3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"## 🧚‍♀️ Welcome to Your Magical Fairy Tale Chatbot!\")\n",
        "    gr.Markdown(\"Talk to classic fairy tales like never before ✨ Ask about plots, characters, morals, and more.\")\n",
        "    gr.Image(\"/content/bg.gif\", height=278, width = 500)\n",
        "    gr.ChatInterface(\n",
        "        fn=respond_to_user,\n",
        "        title=\"🧚 Fairy Tale RAG Chatbot\",\n",
        "        description=\"Ask anything about your favourite fairy tales!\",\n",
        "        examples=[\"Does the little mermaid sing?\", \"Who helped Rapunzel escape?\"],\n",
        "        type=\"messages\"\n",
        "        )\n",
        "demo.launch(share=True, inline=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_PrdLMELmaV",
        "outputId": "e8046c2a-1bf8-4499-f78a-a152ef29488b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f79d989e09cc498a5a.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = qa_chain.invoke({\"question\": 'Tell me about the little mermaid'})"
      ],
      "metadata": {
        "id": "iikMIqvJu5Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sg7P3s2XbDn",
        "outputId": "0702f9e9-2b79-491d-ad6f-04c0396d091b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'Tell me about the little mermaid',\n",
              " 'chat_history': [HumanMessage(content='Does the little mermaid sing?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Yes, the little mermaid is described as having a lovely and sweet voice. However, she gives up her voice to the sea witch in order to obtain legs and be with the human prince she loves.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='tell me more about the witch', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='In \"The Little Mermaid\" story by Hans Christian Andersen, the sea witch is a significant character who plays a crucial role in the mermaid\\'s journey. She is an outcast from the rest of the sea kingdom, living in a dark and gloomy region filled with polyps. The sea witch is known for her power and knowledge of magic, which she uses to help the mermaid in exchange for something valuable.\\n\\nThe mermaid visits the sea witch to request legs so she can be with the human prince she has fallen in love with. The sea witch agrees to the arrangement but extracts a heavy price: the mermaid must give up her tongue and beautiful voice, and if the prince marries someone else, she will turn into sea foam. The sea witch also provides a potion that will make the mermaid\\'s new legs feel as if she\\'s dancing on knives, causing her pain with every step she takes.\\n\\nThe sea witch is depicted as a cunning and sinister character who is not above taking advantage of the mermaid\\'s desperation. She is aware of the consequences of her actions and is unbothered by the pain and suffering they may cause. Despite her dark and menacing nature, the sea witch is a fascinating and memorable character in the story of \"The Little Mermaid.\"', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='tell me about cinderella', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Certainly! The Cinderella story is a classic fairy tale that has been told in various versions throughout the world. The version I\\'m referring to is the one collected by the Grimm Brothers, titled \"Aschenputtel.\"\\n\\nThe story revolves around Cinderella, a kind and gentle girl who is mistreated by her cruel stepmother and stepsisters. Despite her hardships, Cinderella remains hopeful and patient. Her daily chores include drawing water, making fires, cooking, washing, and even picking up peas and lentils strewn among the ashes. At night, she has no bed and must rest on the hearth among the cinders, earning her the nickname \"Cinderella.\"\\n\\nOne day, the King\\'s son hosts a ball, and Cinderella\\'s stepsisters are invited. Although Cinderella is not allowed to attend due to her dirty appearance, she expresses her desire to go. With the help of a magical tree and bird, Cinderella receives a beautiful gown and glass slippers, enabling her to attend the ball. At the ball, she captivates the Prince, but she leaves before midnight, forgetting one of her glass slippers behind.\\n\\nThe Prince, determined to find his true love, searches for the woman whose foot fits the slipper. Eventually, he finds Cinderella, and they are married, living happily ever after. In this story, the stepsisters are punished for their wickedness and falsehood, while Cinderella\\'s kindness and patience are rewarded.\\n\\nSome notable elements of the Grimm Brothers\\' version include the magical tree and bird that grant Cinderella\\'s wishes, the golden slipper that helps the Prince find his true love, and the poetic justice served to the stepsisters in the end. The moral of the story emphasizes the importance of staying kind and true to oneself, as goodness and patience are always rewarded.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Tell me about the little mermaid', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"The Little Mermaid is the protagonist of Hans Christian Andersen's fairy tale of the same name. She is the youngest princess in the sea kingdom, where she is known for her exceptional beauty and kind heart. The Little Mermaid is fascinated by the human world and longs to become human herself, leading her to make a dangerous deal with the Sea Witch.\\n\\nThe Little Mermaid is known for her selflessness and devotion, particularly in her love for the human prince she saves from drowning. She is willing to give up her voice and undergo great pain to be with him, and she ultimately sacrifices herself for his happiness. Despite the challenges she faces, the Little Mermaid remains hopeful and optimistic, always looking for the best in people and situations.\\n\\nThroughout the story, the Little Mermaid is also depicted as being graceful, elegant, and artistic, particularly in her dancing. She is admired for her beauty and poise, and she uses her talents to bring joy to others.\\n\\nOverall, the Little Mermaid is a complex and nuanced character who embodies many admirable qualities, making her one of Hans Christian Andersen's most beloved creations.\", additional_kwargs={}, response_metadata={})],\n",
              " 'answer': \"The Little Mermaid is the protagonist of Hans Christian Andersen's fairy tale of the same name. She is the youngest princess in the sea kingdom, where she is known for her exceptional beauty and kind heart. The Little Mermaid is fascinated by the human world and longs to become human herself, leading her to make a dangerous deal with the Sea Witch.\\n\\nThe Little Mermaid is known for her selflessness and devotion, particularly in her love for the human prince she saves from drowning. She is willing to give up her voice and undergo great pain to be with him, and she ultimately sacrifices herself for his happiness. Despite the challenges she faces, the Little Mermaid remains hopeful and optimistic, always looking for the best in people and situations.\\n\\nThroughout the story, the Little Mermaid is also depicted as being graceful, elegant, and artistic, particularly in her dancing. She is admired for her beauty and poise, and she uses her talents to bring joy to others.\\n\\nOverall, the Little Mermaid is a complex and nuanced character who embodies many admirable qualities, making her one of Hans Christian Andersen's most beloved creations.\",\n",
              " 'source_documents': [Document(metadata={'author': '', 'creator': 'Microsoft Word', 'total_pages': 19, 'title': 'TALE056.PDF', 'producer': 'Acrobat PDFWriter 3.0 for Windows', 'source': '/content/The little mermaid.pdf', 'keywords': '', 'subject': '', 'creationdate': 'Wednesday, September 03, 1997 7:49:07 AM', 'page_label': '1', 'page': 0}, page_content='1872\\nFAIRY TALES OF HANS CHRISTIAN ANDERSEN\\nTHE LITTLE MERMAID\\nHans Christian Andersen\\nAndersen, Hans Christian (1805-1875) - A Danish writer who is\\nremembered as one of the world’s greatest story-tellers. Although\\nmost of his poems, novels, and dramas have been forgotten, his\\nFairy Tales, (compiled 1835- 1872), have gained him lasting fame.\\nThe Little Mermaid - One of Hans Christian Andersen’s Fairy\\nTales. The Sea King’s youngest daughter longs to be human and to'),\n",
              "  Document(metadata={'page_label': '14', 'keywords': '', 'subject': '', 'title': 'TALE056.PDF', 'producer': 'Acrobat PDFWriter 3.0 for Windows', 'creationdate': 'Wednesday, September 03, 1997 7:49:07 AM', 'author': '', 'creator': 'Microsoft Word', 'page': 13, 'source': '/content/The little mermaid.pdf', 'total_pages': 19}, page_content='“Oh if he could only know that! I have given away my voice\\nforever, to be with him.” The slaves next performed some pretty\\nfairy-like dances, to the sound of beautiful music. Then the little\\nmermaid raised her lovely white arms, stood on the tips of her\\ntoes, and glided over the floor, and danced as no one yet had been\\nable to dance. At each moment her beauty became more revealed,\\nand her expressive eyes appealed more directly to the heart than'),\n",
              "  Document(metadata={'creator': 'Microsoft Word', 'source': '/content/The little mermaid.pdf', 'producer': 'Acrobat PDFWriter 3.0 for Windows', 'page': 17, 'creationdate': 'Wednesday, September 03, 1997 7:49:07 AM', 'title': 'TALE056.PDF', 'total_pages': 19, 'keywords': '', 'author': '', 'subject': '', 'page_label': '18'}, page_content='blood. She cast one more lingering, half-fainting glance at the\\nprince, and then threw herself from the ship into the sea, and\\nthought her body was dissolving into foam. The sun rose above the\\nwaves, and his warm rays fell on the cold foam of the little\\nmermaid, who did not feel as if she were dying. She saw the bright\\nsun, and all around her floated hundreds of transparent beautiful\\nbeings; she could see through them the white sails of the ship, and'),\n",
              "  Document(metadata={'title': 'TALE056.PDF', 'page_label': '17', 'creationdate': 'Wednesday, September 03, 1997 7:49:07 AM', 'keywords': '', 'source': '/content/The little mermaid.pdf', 'producer': 'Acrobat PDFWriter 3.0 for Windows', 'subject': '', 'total_pages': 19, 'author': '', 'page': 16, 'creator': 'Microsoft Word'}, page_content='awaited her: she had no soul and now she could never win one. All\\nwas joy and gayety on board ship till long after midnight; she\\nlaughed and danced with the rest, while the thoughts of death\\nwere in her heart. The prince kissed his beautiful bride, while she\\nplayed with his raven hair, till they went arm-in-arm to rest in the\\nsplendid tent. Then all became still on board the ship; the\\nhelmsman, alone awake, stood at the helm. The little mermaid'),\n",
              "  Document(metadata={'creator': 'Microsoft Word', 'producer': 'Acrobat PDFWriter 3.0 for Windows', 'page_label': '16', 'source': '/content/The little mermaid.pdf', 'page': 15, 'subject': '', 'total_pages': 19, 'creationdate': 'Wednesday, September 03, 1997 7:49:07 AM', 'keywords': '', 'author': '', 'title': 'TALE056.PDF'}, page_content='being brought up and educated in a religious house, where she\\nwas learning every royal virtue.\\nAt last she came. Then the little mermaid, who was very anxious to\\nsee whether she was really beautiful, was obliged to acknowledge\\nthat she had never seen a more perfect vision of beauty. Her skin\\nwas delicately fair, and beneath her long dark eye-lashes her\\nlaughing blue eyes shone with truth and purity.\\n“It was you,” said the prince, “who saved my life when I lay dead')]}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oq_nfTvzXdLo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}